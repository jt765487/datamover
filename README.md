# Data Mover Application - System Description (v2)

## System Overview

This system consists of multiple instances of an **External Data Generator** application producing `.pcap` files and corresponding metadata, and a central **Data Mover Application** responsible for reliably collecting, processing, and uploading these `.pcap` files to a remote NiFi host. The entire system runs on Oracle Linux 9, with the external applications managed by Systemd.

## External Data Generator Application

* **Instances:** A small number of independent instances run concurrently, managed by `Systemd`.
* **Output Directories:**
    * `Source Directory`: A shared directory where *all* instances write their `.pcap` data files.
    * `CSV Directory`: A shared directory where *each* instance creates and manages its *own unique* `.csv` metadata file.
* **Workflow (per instance):**
    1.  **Startup:** Creates (or truncates if existing) its dedicated `.csv` file in the `CSV Directory`.
    2.  **PCAP Generation:** Opens a new `.pcap` file in the `Source Directory`.
    3.  **Data Writing:** Writes network packet capture data to the `.pcap` file for approximately 15 seconds.
    4.  **PCAP Closure:** Closes the `.pcap` file.
    5.  **CSV Signaling:** Appends a single line entry to its *own* `.csv` file in the `CSV Directory`. This line acts as a signal that the corresponding `.pcap` file is complete and ready for processing.
        * **Format:** Each line, after stripping leading/trailing whitespace, must strictly adhere to the format:
            `{timestamp},{filepath},{sha256_hash}`
        * **Components:**
            * `{timestamp}`: A non-negative integer (e.g., `1678886400`).
            * `{filepath}`: A non-empty string representing the path to the associated `.pcap` file (e.g., `/path/to/source/data_20230315T120000Z.pcap`). Whitespace within the path is allowed, but leading/trailing whitespace around the path itself (between commas) is ignored. The `parse_log_line` function splits by the first comma, then `rsplit` by the last comma, implying the filepath field itself *can* contain commas.
            * `{sha256_hash}`: A exactly 64-character hexadecimal string (0-9, a-f, A-F), case-insensitive (e.g., `e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855`).
        * **Separators:** Exactly two commas must be present in the line to separate these three fields.
    6.  **Loop:** Repeats steps 2-5 continuously, generating a new `.pcap` file and CSV entry roughly every 15 seconds.
    7.  **Shutdown:** Stops generating files when the `Systemd` service is stopped.
* **Known Issue ("Stuck Files"):** A bug exists where an instance might occasionally fail to close a `.pcap` file, causing it to grow indefinitely. Manual intervention (stopping/restarting the specific `Systemd` service) is currently required to fix this.

## Data Mover Application

* **Goal:** To reliably detect, acquire, upload, and track all `.pcap` files generated by the external applications, ensuring no data loss under normal operation. Runs as a **single process** with multiple threads.
* **Input Directories:** Monitors the `Source Directory` and the `CSV Directory`.
* **Processing Directories:** Uses intermediate and final directories:
    * `Work Directory`: A staging area where `.pcap` files are moved before upload attempts.
    * `Uploaded Directory`: Final location for successfully uploaded files.
    * `Dead Letter Directory`: Final location for files deemed permanently un-uploadable.

### Configuration Management

* The application is configured via an INI-style configuration file (specified by a command-line argument, e.g., `--config app.ini`).
* The configuration file is parsed and validated at startup. Errors in the configuration (missing sections/options, invalid values, type mismatches, cross-field validation failures) will prevent the application from starting.
* **Sections and Key Parameters:**
    * `[Directories]`:
        * `base_dir`: The root directory under which operational subdirectories (`source`, `worker`, `uploaded`, `dead_letter`, `csv`) are automatically created/expected. Must be on a single filesystem.
        * `logger_dir`: Path for log files (must exist prior to startup).
    * `[Files]`:
        * `pcap_extension_no_dot`: File extension for PCAP files (e.g., `pcap`).
        * `csv_extension_no_dot`: File extension for CSV files (e.g., `csv`).
    * `[Mover]`: *(Corresponds to the `FileMoveThread`)*
        * `move_poll_interval_seconds`: Polling interval for the `FileMoveThread` when getting items from the `move_queue`.
    * `[Scanner]`:
        * `scanner_check_seconds`: How often the Source Directory scan runs. *(Note: INI file example uses `scanner_check_seconds` which is a typo).*
        * `lost_timeout_seconds`: Threshold for marking a file as "lost".
        * `stuck_active_file_timeout_seconds`: Threshold for marking a file as potentially "stuck" (must be > `lost_timeout_seconds`).
    * `[Tailer]`: *(Relates to the `TailConsumerThread` polling the `tailer_queue`)*
        * `event_queue_poll_timeout_seconds`: Polling interval for the `TailConsumerThread`.
    * `[Uploader]`:
        * `uploader_poll_interval_seconds`: How often the Work Directory scan runs.
        * `heartbeat_target_interval_s`: How often the uploader logs heartbeat messages.
        * `remote_host_url`: Full URL for the NiFi endpoint.
        * `request_timeout`: Network timeout for upload requests.
        * `verify_ssl`: Whether to validate NiFi host's SSL certificate.
        * `initial_backoff`, `max_backoff`: Parameters for the upload retry delay mechanism.
* The `base_dir` and `logger_dir` paths specified in the INI file can use `~` for user home directory expansion (handled during config loading).
* The application uses an internal `FS` abstraction layer for filesystem operations, facilitating testing.
* **Configuration Rollout & Versioning:** The process for rolling out configuration changes and managing compatibility between INI file versions and application code is currently TBD. Hot-reloading of configuration is not supported; a restart is required for changes to take effect.

### Startup Sequence & Validation

1.  **Platform Check:** Verifies the application is running on Linux. Exits immediately if not.
2.  **Argument Parsing:** Parses command-line arguments (primarily the `--config` file path and an optional `--dev` flag).
3.  **Configuration Loading:** Reads and parses the specified INI configuration file. Performs validation of sections, options, data types, and inter-field constraints (e.g., `stuck_active_file_timeout_seconds` > `lost_timeout_seconds`). Exits on critical errors.
4.  **Logging Setup:** Configures file-based logging (to `DEBUG` level in the configured `logger_dir`) and console logging (`INFO` level by default, `DEBUG` if `--dev` flag is present). Exits on logging setup errors. From this point, subsequent actions are logged.
5.  **Context Building (`build_context`):** Creates the central `AppContext` object. This involves instantiating default implementations (or using test overrides) for the Filesystem abstraction (`FS`), HTTP client (`HttpClient`), and file scanning function (`FileScanner`), and bundling them with the loaded `Config` object and a shared `threading.Event` for shutdown signaling. Exits on errors.
6.  **Signal Handling:** Installs signal handlers for **`SIGINT` and `SIGTERM`**. When received, these handlers trigger the `shutdown_event` within the `AppContext`, initiating a graceful shutdown across all components listening to the event. Exits on errors.
7.  **Directory Validation/Creation:**
    * Validates that the `base_dir` specified in the config exists and is accessible. Exits if not.
    * Ensures the required subdirectories (`source`, `worker`, `uploaded`, `dead_letter`, `csv`) exist under `base_dir`, creating them if necessary (`mkdir -p` equivalent).
    * Validates that all these directories reside on the *same filesystem device* as `base_dir` using `os.stat().st_dev`. Exits on errors or device mismatch.
8.  **Application Execution:** Calls the main `run(context)` function, which starts the monitoring loops, processing threads, and the internal health check loop.

### Application Architecture Overview

The Data Mover application is multi-threaded **within a single process**, orchestrating several components that run concurrently. The main components are:

* **Configuration Loader:** Reads and validates the `app.ini` file at startup.
* **Context Builder:** Initializes and holds shared resources like configuration, filesystem access, HTTP client, and the shutdown event.
* **Directory Validator:** Ensures required directories exist and are correctly configured at startup.
* **Signal Handler:** Catches termination signals (SIGTERM, SIGINT) to initiate graceful shutdown.
* **Main Orchestrator (`app.run`):** Starts and coordinates the following long-running components/threads, and also runs an internal health check loop (see Health & Liveness section).
    * **`directory_scanner` Thread (`ScanThread`):**
        * **Purpose:** Periodically scans the `Source Directory` to identify `.pcap` files missed by the primary CSV trigger ("lost" files) and files exhibiting the external application bug ("stuck active" files).
        * **Frequency:** Runs every `scanner_check_seconds` as defined in the configuration.
        * **State Management:** Maintains a persistent state (`_current_file_states`) between scan cycles. This state tracks each file's path, size, modification time (wall clock), the monotonic time it was first seen, and its size/mtime from the *previous* scan cycle. This allows detection of changes between scans.
        * **Scan Cycle (`DoSingleCycle`):** In each cycle:
            1.  Scans the `Source Directory` for files matching the `.pcap` extension.
            2.  Compares the found files with the state from the previous cycle using `process_scan_results`.
            3.  Identifies files that are "lost" (stale based on `mtime` and `lost_timeout`) or "stuck active" (present longer than `stuck_active_timeout` via monotonic time *and* active since the last scan).
            4.  Identifies files that were previously tracked but are now missing.
        * **Actions & Output:**
            * **Lost Files:** Paths of files newly identified as "lost" in a cycle are put onto the central `move_queue` for the `file_mover` thread to process. A summary log is generated (`report_state_changes`).
            * **Stuck Active Files:** Files newly identified as "stuck active" trigger a **CRITICAL** log message within the `ScanThread` itself (intended for alerting/monitoring). These files are *not* queued for moving. A summary log is also generated (`report_state_changes`).
            * **Removed Files:** Files that disappear from the scan (and were previously tracked) are logged for informational purposes (`report_state_changes`).
        * **Error Handling:** Critical errors during directory scanning (`ScanDirectoryError`) will stop the thread. Other internal processing errors are logged, and the thread attempts to continue, potentially skipping state updates for the failed cycle.
    * **`observer` Thread & `csv_tail_consumer` Thread (`TailConsumerThread`):**
        * Work together to monitor the `CSV Directory`.
        * The `observer` likely uses `inotify` (via `watchdog`) to detect file changes/new files and potentially passes events to the `csv_tail_consumer` via an internal `tailer_queue`.
        * The `csv_tail_consumer` reads new lines from detected/changed `.csv` files, parses them, validates the format, and puts the paths of successfully parsed `.pcap` files onto the central `move_queue`.
    * **`file_mover` Thread (`FileMoveThread`):**
        * **Purpose:** Acts as the dedicated consumer for the central `move_queue`. Its sole responsibility is to take file paths placed on the queue (by either the CSV tailer or the directory scanner) and attempt to move them from the `Source Directory` to the `Work Directory`.
        * **Mechanism:**
            1.  Polls the `move_queue` for incoming `Path` objects.
            2.  For each path received, it calls an injected processing function (`process_single`).
            3.  By default, this function wraps `move_file_safely_impl`, which performs several steps:
                * **Validation:** Checks if the source file exists and is accessible. Optionally validates if it resides within the expected `Source Directory`.
                * **Destination Conflict Resolution:** If the target filename (original source filename) already exists in the `Work Directory`, the `determine_destination` function is used. This attempts to find an available unique filename by appending an incrementing counter suffix (e.g., `filename-1.ext`, `filename-2.ext`, etc., via `find_available_target_path`) up to a configurable limit (currently 100 attempts). If a unique name cannot be found within the limit, or if an OS error occurs during checking, the move for that file is aborted.
                * **Execution:** Performs the actual filesystem move (`mv`).
            4.  Logs the outcome (success or failure) of the move attempt.
        * **Decoupling:** This thread decouples the *detection* of files needing processing from the *action* of moving them into the `Work Directory`, ensuring moves are serialized and handled consistently.
        * **Error Handling:** Includes basic error handling for queue operations and logs unexpected errors occurring within the file moving logic itself. `move_file_safely_impl` handles and logs specific validation or execution errors, returning `None` on failure.
        * **Shutdown:** Listens for the `stop_event` and exits its loop cleanly when the event is set.
    * **`uploader` Thread (`UploaderThread`):**
        * Periodically scans the `Work Directory` based on `uploader_poll_interval_seconds`.
        * Attempts to upload `.pcap` files found there to the NiFi host using the `RetryableFileSender` logic (retries, backoff).
        * Moves successfully uploaded files to the `Uploaded Directory`.
        * Moves terminally failed files to the `Dead Letter Directory`.
* **Shared Queues:** Bounded `queue.Queue` instances (`move_queue`, `tailer_queue`) are used for communication between threads, decoupling detection/identification from action (moving/processing).
    * **Queue Full Behavior (`safe_put`):**
        * `safe_put` is used for enqueuing items. Its behavior depends on the `timeout` parameter: `None` (default) blocks indefinitely, `>0` blocks for that duration, `<=0` is non-blocking.
        * If a queue is full and the timeout expires (or for a non-blocking put), `QueueFullOnPut` is raised and logged by the caller of `safe_put`. Other unexpected errors during `put` raise `QueuePutFailed`.
        * Producers to the `move_queue` (CSV tailer, directory scanner) typically use `safe_put` in a way that blocks indefinitely. This implies that if the `file_mover` thread cannot keep up, these producer threads will stall, indicating a significant downstream bottleneck.
        * The `MappingEventHandler` (producing to `tailer_queue`) uses `safe_put` with a configurable timeout (or blocks indefinitely). If the `tailer_queue` fills and a timeout occurs, `inotify` events might be delayed or effectively lost, and an error is logged by the handler.
        * Persistently full queues are considered indicative of a critical application state requiring investigation, rather than a condition the application attempts to manage with complex internal backpressure logic beyond basic error logging and the inherent blocking of producers.
* **Purge Component:** *(Defined by strategy, likely runs as a separate thread or external job, not shown in `app.run`. **This is a planned future solution, not yet implemented.** The decision between a built-in thread or external cron job for its implementation is TBD.)*. Manages disk space according to the defined purging strategy.

### Workflow & Mechanisms

1.  **Primary Trigger (CSV Monitoring):**
    * **Mechanism:** Uses the `watchdog` library (interfacing with `inotify`) to detect changes within the `CSV Directory`. A dedicated event handler (`MappingEventHandler`) filters events for `.csv` files directly within the directory.
    * **Initial Scan:** On startup, scans the `CSV Directory` for pre-existing `.csv` files and processes them (`InitialFoundEvent`).
    * **Stateful Tailing (`TailProcessor`):** Maintains state (last read position, partial line buffer) for each tracked `.csv` file:
        * **Append Handling:** On file growth, reads only new bytes from the last position.
        * **Truncation Handling:** On file shrink, resets position to the new EOF, clears buffer.
        * **Line Buffering:** Extracts complete `\n`-terminated lines, decodes as UTF-8 (replacing errors), strips whitespace. Keeps partial lines buffered.
        * **Move/Delete Handling:** Stops tracking deleted/moved-out files. Treats moved-in files as new.
    * **Parsing & Output:** Each complete line is parsed (`parse_log_line`). If valid, the extracted `.pcap` path is put onto the central `move_queue`. Errors are logged.
    * **Event Flow:** OS Event -> `watchdog` -> `MappingEventHandler` -> `event_queue` -> `TailConsumerThread` -> `TailProcessor` -> `parse_log_line` -> `move_queue`.
    * *Reliability Note: Fundamentally relies on OS event notifications, which are not guaranteed 100%.*

2.  **File Acquisition (`file_mover` Thread):** This thread monitors the central `move_queue`. When a path appears (from either the CSV trigger or the source scanner), it moves the corresponding file from the `Source Directory` to the `Work Directory` using `move_file_safely_impl`.

3.  **Processing Queue (`uploader` Thread):**
    * Periodically (every `uploader_poll_interval_seconds`), the `uploader` thread scans the `Work Directory`.
    * Identifies all `.pcap` files residing there.
    * Files are queued internally for upload attempt, ordered from oldest to newest (based on filesystem modification time).

4.  **Upload Attempt & Retry Logic (`RetryableFileSender`):**
    * Takes the next file from the internal processing queue.
    * Attempts to POST the file to the configured NiFi host (`remote_host_url`).
    * **Reliability Core:** If the upload fails due to network errors (`Timeout`, `ConnectionError`) or server errors (5xx status codes), the file remains in the `Work Directory` and will be retried after an exponential backoff delay (configured by `initial_backoff`, `max_backoff`). This retry loop continues indefinitely by default.

5.  **Upload Success:**
    * If the upload to NiFi succeeds (2xx status code), the file is moved (`mv`) from the `Work Directory` to the `Uploaded Directory` using `move_file_safely_impl`.

6.  **Upload Permanent Failure ("Dead Letter"):**
    * If an upload attempt results in a *terminal (non-retryable)* failure, the file is moved (`mv`) from the `Work Directory` to the `Dead Letter Directory`. Conditions include:
        * **Non-Retryable HTTP Status Code:** NiFi responds with a non-2xx, non-5xx code (typically **4xx Client Errors**).
        * **Non-Retryable HTTP Client Exception:** Errors like SSL issues, malformed URLs, etc. (`requests.exceptions.RequestException` subtypes excluding network errors).
        * **File System Error During Upload:** `OSError` while opening/reading the source file during the upload attempt.
        * **Unexpected Error During Upload:** Any other Python `Exception` during the POST process.
    * *Critical Failure Note: If moving the file to the `Dead Letter Directory` *also* fails, a CRITICAL error is logged.*

7.  **Orphan File Detection (Backup Trigger - `directory_scanner` Thread):**
    * To compensate for the primary trigger's potential unreliability, the `directory_scanner` thread periodically scans the `Source Directory`.
    * It looks for "lost" `.pcap` files.
    * **Definition:** A file is **"lost"** if it was present in the previous scan and its wall clock `mtime` is older than `lost_timeout`. (New files aren't checked initially).
    * Detected "lost" files have their paths **put onto the central `move_queue`** to be moved by the `file_mover` thread into the `Work Directory`.

8.  **Stuck File Detection (`directory_scanner` Thread):**
    * The periodic scan also identifies "stuck active" files resulting from the external app bug.
    * **Definition:** A file is **"stuck active"** if it meets **both**:
        1.  It has been observed for longer than `stuck_active_timeout` (using **monotonic clock**).
        2.  It has changed size or `mtime` since the *previous* scan.
    * **Current Action:** Logs a **CRITICAL** error message via the `ScanThread` identifying the newly stuck active file. The file is *not* moved or queued.
    * **Future TODO:** Trigger restart of the external app via `Systemd`.

9.  **Data Purging (Planned Strategy):**
    * An independent process (e.g., scheduled job, separate thread - TBD, **this is a planned future solution, not yet implemented**. The decision between a built-in thread or external cron job for its implementation is TBD.) will periodically manage disk usage to prevent exceeding a threshold (e.g., 80%).
    * **Purge Logic:**
        * **Dead Letter Cleanup:** Delete files in `Dead Letter Directory` older than a fixed period (e.g., 48 hours).
        * **Capacity Management (if disk > 80%):**
            1.  Calculate space needed.
            2.  Delete oldest files from `Uploaded Directory` until target met or directory empty.
            3.  If still over target, delete oldest files from `Work Directory` until target met.
    * *Note: Prioritizes retaining newer data during prolonged upload outages.*

### State Management & Recovery

* The application is designed to be stateless regarding its processing pipeline; the **filesystem is the sole source of truth** for file states. No external database or state file is used to track progress.
* Atomic `mv` operations (ensured by the same-filesystem constraint) guarantee that a file exists in only one of the primary directories (`Source`, `Work`, `Uploaded`, `Dead Letter`) at any given time.
* **Recovery from Abnormal Shutdown (e.g., `kill -9`, crash):** Upon restart:
    * Files remaining in the `Source Directory` will be picked up by the `directory_scanner` during its next cycle (potentially as "lost" files) and enqueued to the `move_queue`.
    * Files present in the `Work Directory` will be scanned by the `uploader` thread and re-attempted for upload.
    * This design means that, at worst, NiFi might receive duplicate uploads if the application crashed after a successful upload but before moving the file from `Work` to `Uploaded`. NiFi is expected to handle such duplicates (deduplication logic on NiFi side is TBD). *(Consideration for future: The Data Mover could embed an idempotency key, e.g., SHA256 or timestamp/filename combination, in the upload request to aid NiFi in deduplication.)*

### Concurrency Control

* The application utilizes multiple threads **within a single process**.
* **Inter-thread Communication:** Uses thread-safe `queue.Queue` instances (`move_queue`, `tailer_queue`) with fixed maximum sizes.
* **`move_queue`:** Centralizes requests to move files from Source to Work, consumed by the single `file_mover` thread, preventing race conditions on source files.
* **Uploader Concurrency:** Only **one** `uploader` thread runs, simplifying state management in the `Work Directory`.
* **Filesystem Operations & Locking:**
    * No explicit file locking mechanisms (e.g., `flock`, `fcntl`) are used by this application or the external data generator.
    * Atomicity of file operations relies on the behavior of the `mv` command (or its Python equivalent used by `fs.rename`) on a Linux system when the source and destination are on the **same filesystem**. This operation is guaranteed to be atomic by the OS. The application validates at startup that all relevant directories are on the same filesystem device.
* **Clock Usage:** Uses wall clock time (`time.time()`, file `mtime`) for inactivity checks ("lost") and monotonic clock time (`time.monotonic()`) for presence duration checks ("stuck active"). *Note: `mtime` values from network filesystems (NFS) are subject to clock synchronization between the NFS server and client.*

### Logging

* **Configuration:** Configured at startup via `logging.config.dictConfig`.
* **Log Directory:** Uses `logger_dir` from `app.ini` (must pre-exist).
* **File Logging:** JSON Lines format (`app.log.jsonl`), `DEBUG` level. Rotates based on size (e.g., 10MB) and count (e.g., 5 backups).
    * **Sample JSON Log Entry:**
      ```json
      {"timestamp": "2023-03-15T12:00:05.123Z", "thread_name": "UploaderThread", "level": "INFO", "message": "Upload SUCCESS for 'file.pcap' (Status: 200). Moving to UPLOADED dir.", "logger": "data_mover.uploader.retryable_file_sender", "function": "send_file", "line": 150}
      ```
* **Console Logging:** Human-readable format to `stderr`, `INFO` level default, `DEBUG` if `--dev` flag used.
* **Level Control:** Specific noisy loggers may be raised to `INFO`.

### Graceful Shutdown

* **Signal Handling:** Signal handlers for `SIGTERM` and `SIGINT` are installed. Receipt of these signals triggers a shared `shutdown_event`.
* **Shutdown Process:**
    * Components check the `shutdown_event` to stop processing new work.
    * Specific components (`observer`, `csv_tail_consumer`) may receive explicit `.stop()` calls first.
    * The main thread attempts to `.join()` all started threads with a timeout (e.g., 5s). This is a best-effort graceful shutdown; as all threads are daemon threads, they will not prevent application exit if the main thread terminates.
* **Queue Draining:** Queues (`move_queue`, `tailer_queue`) are not explicitly drained upon shutdown. Items remaining in queues when threads terminate due to the `stop_event` will not be processed in the current run. They will be re-discovered upon application restart via directory scans.
* **In-flight Uploads:** If an upload is in progress when the `stop_event` is set, the `RetryableFileSender` will complete the current HTTP request. If it's a retryable error, the backoff wait will be interrupted by the `stop_event`, and the file will remain in the `Work Directory`.

### Security Context

* **NiFi Communication:** Currently **HTTP only**. No authentication implemented. *(Future TODO: Implement HTTPS and associated authentication. Credential storage and injection methods for this are TBD, common options include environment variables, mounted secrets/files, or integration with a secrets manager like HashiCorp Vault.)*
* **Filesystem Permissions:** Requires appropriate permissions for its service user on configured directories.
* **Path Sanitization:** File paths extracted from CSV files are used to locate files within the `Source Directory`. The `move_file_safely_impl` function, when moving files, validates that the source path is within the `expected_source_dir` (which is the `Source Directory`). This check, combined with resolving the path, mitigates risks of path traversal attacks originating from malicious CSV content during the move operation.

### Monitoring, Metrics & Alerting

* **Current Approach:** Primary monitoring relies on application logs. It is planned that these logs (JSON from file, `stderr` from console) will be ingested into a **Splunk-based solution** for aggregation, searching, and alerting.
* **Alerting Thresholds (via Splunk):** Specific alerting rules will be configured in Splunk based on:
    * `CRITICAL` log messages (e.g., stuck files, failure to move to dead-letter, unexpected thread termination).
    * Persistently full queues (indicated by repeated `QueueFullOnPut` warnings).
    * High rates of `ERROR` or `WARNING` messages.
    * Prolonged absence of heartbeat logs from key components.
* **Application Metrics (Future Consideration):** Direct instrumentation for metrics (e.g., via Prometheus client library or StatsD) is not currently implemented but may be considered in the future. Potential metrics include:
    * Queue depths (`move_queue`, `tailer_queue`, internal uploader queue).
    * Scan cycle latencies (`directory_scanner`).
    * File processing rates (files moved/sec, files uploaded/sec).
    * Upload success/failure/retry counts.
    * Upload latencies.
    * Number of "lost" or "stuck active" files detected per cycle.
    * Disk usage percentages for monitored directories.

### Health & Liveness

* **Internal Thread Monitoring:** The main application loop (within `app.run`) will periodically (e.g., every 30 seconds) check if all critical worker threads (`directory_scanner`, `observer`, `csv_tail_consumer`, `file_mover`, `uploader`) are alive.
* **Failure Detection & Action:** If any of these critical threads is found to be not alive, a `CRITICAL` message will be logged, and the application will call `sys.exit(1)`. This deliberate termination allows `Systemd` (or another process supervisor) to restart the entire application, leveraging its stateless design for recovery.
* **Health Endpoint:** The application does not expose a dedicated HTTP health endpoint (e.g., `/healthz`).
* **Process Supervision:** `Systemd` is responsible for monitoring the application process and restarting it if it dies or exits due to the internal health check.

### SLAs / SLOs & Business Requirements (Targets)

* **Availability:** The application is expected to run 24x7.
* **Data Integrity:** **No data loss** of `.pcap` files is the primary requirement. Files must be reliably moved from source to their final destination (Uploaded or Dead Letter).
* **Processing Order:** Files should generally be uploaded in the order they were created by the external applications. This is supported by the `UploaderThread` scanning its `Work Directory` and processing files oldest-first by modification time.
* **Latency:** Processing latency (time from CSV signal/file creation to successful upload) should be as low as possible. Specific SLOs are TBD.
* **NiFi Downtime Handling:** If NiFi is unavailable, files will buffer in the `Work Directory`. The application will continue to retry uploads. If disk space becomes an issue, the (future) purge process will prioritize deleting older files from `Uploaded` and then `Work` directories.
* **Burst Rate Protection:** The primary protection against overwhelming NiFi during normal operation is the single `UploaderThread` and its retry/backoff mechanism. If NiFi is down, files buffer locally; there's no specific mechanism to pause external data generation.

### Explicit Assumptions & Dependencies

* **Assumptions:**
    * **Same Filesystem:** All operational directories *must* be on the same filesystem device (validated at startup).
    * **OS Event Reliability:** Relies on reasonably reliable OS filesystem event notifications.
    * **Clock Behavior:** Standard wall and monotonic clock behavior. `mtime` values from network filesystems (NFS) are subject to clock synchronization between the NFS server and client.
    * **NiFi Endpoint:** Available and accepts basic POST requests.
* **Dependencies:**
    * **Operating System:** **Linux**.
    * **Python:** Version **3.13 or higher**. The RPM packaging process will ensure this version is available for the application, potentially by bundling it within a virtual environment.
    * **Libraries:** `watchdog`, `requests`.
    * **External Services:** NiFi host, `watchman` (if used by `watchdog` backend), functioning filesystem.

### SHA256 Hash Usage

* The external app provides a SHA256 hash in the CSV.
* The parser validates its format.
* **Current Status:** The hash is **not currently used** for integrity checks or passed to NiFi.

### Performance & Volume (Estimates)

* **Data Volume:** Approx. **1 TB** per day.
* **File Rate:** The combined input rate from external applications is approximately **12 files per minute**. This translates to the target processing rate for the `move_queue`.
* **Queue Behavior Note:** The system is designed such that queues (`move_queue`, `tailer_queue`) should not fill up under normal operation. A persistently full queue indicates a significant downstream processing bottleneck (e.g., `FileMoveThread` or `UploaderThread` cannot keep up, or NiFi is unavailable) and represents a critical application state that may lead to stalling or data loss (for `tailer_queue` if timeouts are used and events are dropped).

### Testing Strategy

* **Framework:** All tests utilize the `pytest` framework. Static analysis with `mypy` (for type checking) and `ruff` (for linting) is also employed.
* **Unit Tests:**
    * Provide good code coverage for individual modules and functions.
    * Examples include testing the `RetryableFileSender` for various HTTP responses (success, retryable errors, terminal errors, network exceptions), source file vanishing scenarios, and backoff logic.
    * Other unit tests cover CSV line parsing, file state record management, and individual helper functions.
* **Integration Tests:**
    * Focus on the interaction between different components of the application.
    * Example: Testing the `DoSingleCycle` processor (core of the `directory_scanner` thread) using a real filesystem (via `tmp_path`) to verify detection of new, lost, stuck, and disappeared files, and correct interaction with the `lost_file_queue`.
    * These tests mock external boundaries where necessary (e.g., `safe_put` to queues, `report_state_changes`) to isolate the component interactions under test.
* **Black-Box End-to-End Tests:**
    * Validate the entire application pipeline from file creation to simulated upload.
    * **Methodology:**
        * The full Data Mover application is started in a separate thread.
        * A real filesystem (temporary directories) is used.
        * The external application behavior is simulated by creating `.pcap` files in the `Source Directory` and appending corresponding lines to `.csv` files in the `CSV Directory`.
        * A local HTTP server (`pytest-httpserver`) simulates the NiFi endpoint, allowing tests to define expected requests (URI, method, headers, data) and configure responses (200 OK, errors).
        * Assertions are made on the final location of files (e.g., in `Uploaded Directory`), the absence of files in intermediate directories, and the HTTP requests received by `pytest-httpserver`.
    * **Coverage:** These tests cover "happy path" scenarios (e.g., successful processing via CSV trigger) and will be expanded to include error conditions and other trigger mechanisms (e.g., "lost file" recovery), NiFi outages, and disk exhaustion scenarios (once purging is implemented).

### Deployment

* **Packaging & Distribution:** The application will be packaged as an **RPM** for deployment on Oracle Linux 9 systems.
    * The RPM will handle the installation of the application code, its Python dependencies (potentially by creating a virtual environment with Python 3.13+ and installing dependencies into it), and the creation of necessary users or groups.
    * It will also install and enable a **`Systemd` service unit file** to manage the application lifecycle (start, stop, restart, status).
* **Target Environment:** Air-gapped production systems running Oracle Linux 9.
* **Configuration Rollout:** The method for rolling out updates to the `app.ini` configuration file is TBD. Common approaches involve configuration management tools (Ansible, Chef, Puppet) or including the configuration within the deployment package. Hot-reloading of configuration is not currently supported; a restart is required.

### Operational Considerations & Runbooks (High-Level)

* **Stuck External Application Instance:**
    * **Symptom:** `CRITICAL` logs from `ScanThread` identifying "STUCK ACTIVE" files.
    * **Action:** Manually restart the corresponding external data generator application instance via `Systemd`. *(Future TODO: Automate this restart via `systemctl` calls from the Data Mover application, if feasible and secure).*
* **`Dead Letter Directory` Backlog:**
    * **Symptom:** Files accumulating in the `Dead Letter Directory`.
    * **Investigation:** Examine application logs for errors associated with these files to understand the cause of terminal failure (e.g., persistent 4xx errors from NiFi, unreadable files).
    * **Action:** Address the root cause (e.g., fix NiFi endpoint issue, correct problematic file). Manually re-process files by moving them back to the `Work Directory` if the issue is resolved. The (future) purge mechanism will eventually delete old dead-letter files.
* **`directory_scanner` Crashed / `Source Directory` Not Processed:**
    * **Symptom:** No "lost file" or "stuck file" logs; files accumulate in `Source Directory` without being moved to `Work`. `Systemd` may report the service as failed if the main process died.
    * **Action:** Check `Systemd` status. Review application logs for crash details. Restart the application via `Systemd`.
* **Disk Full / Approaching Full:**
    * **Symptom:** `OSError: No space left on device` in logs; alerts from disk monitoring.
    * **Action:** The (future) purge mechanism is intended to prevent this. If it occurs before purging is implemented or if purging cannot keep up:
        1.  Manually clear space from `Uploaded Directory` (oldest first).
        2.  If still an issue, clear space from `Work Directory` (oldest first, understanding these are unprocessed).
        3.  Investigate why NiFi might be down or why processing is stalled, leading to `Work Directory` buildup.
* **Missed CSV Entries / Files Not Processed via CSV:**
    * **Symptom:** Files appear in `Source Directory` and are only picked up by the "lost file" scan, not immediately via CSV.
    * **Investigation:** Check `TailConsumerThread` and `MappingEventHandler` logs for errors related to CSV processing or `inotify` event handling. Ensure `watchdog` and its backend are functioning correctly.
    * **Action:** If a batch of CSV entries was definitively missed, and the files are still in `Source`, the "lost file" scan will eventually process them. If files were moved or deleted before the "lost file" scan, manual intervention might be needed to re-introduce them or their CSV signals if recovery is critical.

### Future Considerations & TBD Items

* Implementation of the **Purge Component**.
* Detailed **Resource Usage Profiling** (CPU, Memory, Network).
* Implementation of **HTTPS and Authentication** for NiFi communication, including secure credential management.
* Definition of specific **SLOs** for latency and error rates.
* Strategy for **CSV format versioning** if new fields are added.
* Potential for the Data Mover to **auto-restart stuck external generator apps**.
* Refinement of **NiFi-side deduplication strategy** and potential for client-side idempotency keys.

